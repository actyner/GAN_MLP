{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpJhs8zTjtPC",
    "outputId": "3863286b-7628-4cf8-b122-b7ae4875bb4f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Input\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import UpSampling2D\n",
    "import keras\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VS3DjudcwZ4R"
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pymatgen.core import Lattice, Structure, Molecule\n",
    "from pymatgen.analysis.structure_matcher import StructureMatcher\n",
    "atom_dict1=['na','H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm','Md','No','Lr','Rf','Db','Sg','Bh','Hs','Mt','Ds','Rg','Cn','Nh','Fl','Mc','Lv','Ts','Og']\n",
    "#removing nobels \n",
    "atom_dict=['na','H','Li','Be','B','C','N','O','F','Na','Mg','Al','Si','P','S','Cl','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm','Md','No','Lr','Rf','Db','Sg','Bh','Hs','Mt','Ds','Rg','Cn','Nh','Fl','Mc','Lv','Ts','Og']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw1RWSXJEvDC"
   },
   "outputs": [],
   "source": [
    "#Load functions for latent points and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hhHjKt9Ln2kZ"
   },
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OH3mwUyzYMle"
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(n_samples):\n",
    "    X=generate_latent_points(512*80,n_samples)\n",
    "    X=X.reshape((n_samples,160,256))/(np.max(np.abs(X)))\n",
    "    X=(X+1)/2\n",
    "    y = np.zeros((n_samples,1))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9ANzO_8ybTwi"
   },
   "outputs": [],
   "source": [
    "def define_generator():\n",
    "    #Generator\n",
    "    visible = keras.layers.Input(shape=(128,))\n",
    "    n_nodes = 128 * 16 * 10\n",
    "    dense = keras.layers.Dense(n_nodes)(visible)\n",
    "    reshape1 = keras.layers.Reshape((10, 16, 128))(dense)\n",
    "    upsample1 = keras.layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(reshape1)\n",
    "    LeakyRelu1 = keras.layers.LeakyReLU(alpha=0.2)(upsample1)\n",
    "    upsample2 = keras.layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(LeakyRelu1)\n",
    "    LeakyRelu2 = keras.layers.LeakyReLU(alpha=0.2)(upsample2)\n",
    "    upsample3 = keras.layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(LeakyRelu2)\n",
    "    LeakyRelu3 = keras.layers.LeakyReLU(alpha=0.2)(upsample3)\n",
    "    upsample4 = keras.layers.Conv2DTranspose(64, (2,2), strides=(1,4), padding='same')(LeakyRelu3)\n",
    "    LeakyRelu4 = keras.layers.LeakyReLU(alpha=0.2)(upsample4)\n",
    "    conv_layer1 = Conv2D(1, (2,2), activation='sigmoid', padding='same')(LeakyRelu4)\n",
    "    #output = crstldisc(conv_layer1)\n",
    "    generator = keras.Model(visible, conv_layer1)\n",
    "    generator.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    #generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AZ-kcVZ6ahcC"
   },
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(128,n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X[:,:,:,0], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvdpZP4sSc0K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-04 18:15:47.209608: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2025-02-04 18:15:47.209636: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 48.00 GB\n",
      "2025-02-04 18:15:47.209643: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 18.00 GB\n",
      "2025-02-04 18:15:47.209660: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-04 18:15:47.209670: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/envs/tf2/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHGNet v0.3.0 initialized with 412,525 parameters\n",
      "CHGNet v0.3.0 initialized with 412,525 parameters\n",
      "CHGNet will run on mps\n"
     ]
    }
   ],
   "source": [
    "#Load atom + lattice decoder, load CHGNet and CGCNN for use as discriminator\n",
    "\n",
    "import atomate2\n",
    "from atomate2.forcefields.flows.phonons import PhononMaker\n",
    "from pymatgen.core.structure import Structure\n",
    "from jobflow import run_locally\n",
    "from pymatgen.phonon.bandstructure import PhononBandStructureSymmLine\n",
    "from pymatgen.phonon.dos import PhononDos\n",
    "from pymatgen.phonon.plotter import PhononBSPlotter, PhononDosPlotter\n",
    "from jobflow import SETTINGS\n",
    "from numpy import genfromtxt\n",
    "import subprocess\n",
    "import os\n",
    "from jobflow import run_locally\n",
    "\n",
    "from pymatgen.phonon.bandstructure import PhononBandStructureSymmLine\n",
    "from pymatgen.phonon.dos import PhononDos\n",
    "from pymatgen.phonon.plotter import PhononBSPlotter, PhononDosPlotter\n",
    "from jobflow import SETTINGS\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import LeakyReLU\n",
    "import keras.backend as K\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)\n",
    "\n",
    "input_img = keras.Input(shape=(64,64,64, 1))\n",
    "x = layers.BatchNormalization()(input_img)\n",
    "x = layers.Conv3D(16, 3, strides = (4,4,4),  padding='same')(input_img)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Conv3D(8, 3, strides = (4,4,4),activation='tanh', padding='same')(x)\n",
    "encoded = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(512, activation=None)(encoded)\n",
    "x = layers.Reshape((4,4,4,8))(x)\n",
    "x = layers.Conv3DTranspose(4, 8, strides=(4,4,4), padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = layers.Conv3DTranspose(8, 8, strides=(4,4,4),  padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "decoded = layers.Conv3D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "#autoencoder.compile(optimizer='adam', loss=K.binary_crossentropy, metrics=['mae'])\n",
    "#autoencoder.summary()\n",
    "apos1 = autoencoder\n",
    "\n",
    "# Restore the weights\n",
    "apos1.load_weights('AtomWeights.weights.h5')\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    a=K.sum(K.abs(K.log(K.abs(y_true[:,31,0,0,0]))-K.log(K.abs(y_pred[:,31,0,0,0])))*2)\n",
    "    b=K.sum(K.abs(K.log(K.abs(y_true[:,0,31,0,0]))-K.log(K.abs(y_pred[:,0,31,0,0])))*2)\n",
    "    c=K.sum(K.abs(K.log(K.abs(y_true[:,0,0,31,0]))-K.log(K.abs(y_pred[:,0,0,31,0])))*2)\n",
    "    ab=K.sum(K.abs(K.log(K.abs(y_true[:,31,31,0,0]))-K.log(K.abs(y_pred[:,31,31,0,0])))*2)\n",
    "    bc=K.sum(K.abs(K.log(K.abs(y_true[:,0,31,31,0]))-K.log(K.abs(y_pred[:,0,31,31,0])))*2)\n",
    "    ac=K.sum(K.abs(K.log(K.abs(y_true[:,31,0,31,0]))-K.log(K.abs(y_pred[:,31,0,31,0])))*2)\n",
    "    loss1=a+b+c+ab+bc+ac\n",
    "    loss2 = K.mean(K.abs(y_true - y_pred))\n",
    "\n",
    "    err=loss1+10*loss2\n",
    "\n",
    "    return err\n",
    "import keras\n",
    "from keras import layers\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)\n",
    "\n",
    "input_img = keras.Input(shape=(32, 32, 32, 1))\n",
    "\n",
    "x = layers.Conv3D(16, 3, strides = (2,2,2), activation='relu', padding='same')(input_img)\n",
    "x = layers.Conv3D(16, 3,strides = (2,2,2), activation='relu', padding='same')(x)\n",
    "x = layers.Conv3D(8, 3, strides = (2,2,2),activation='tanh', padding='same')(x)\n",
    "encoded = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(512, activation='relu')(encoded)\n",
    "x = layers.Reshape((4,4,4,8))(x)\n",
    "x = layers.Conv3DTranspose(4, 8, strides=(2,2,2), activation='relu', padding='same')(x)\n",
    "x = layers.Conv3DTranspose(8, 8, strides=(2,2,2), activation='relu', padding='same')(x)\n",
    "x = layers.Conv3DTranspose(16, 16, strides=(2,2,2), activation='relu', padding='same')(x)\n",
    "decoded = layers.Conv3D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "#autoencoder.compile(optimizer='adam', loss=custom_loss, metrics=[custom_loss,'mae'])\n",
    "#autoencoder.summary()\n",
    "apos=autoencoder\n",
    "\n",
    "# Restore the weights\n",
    "apos.load_weights('LatticeWeights.weights.h5')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import keras\n",
    "from keras import layers\n",
    "#apos1 = keras.models.load_model('Atom_Encoder.keras')\n",
    "full_input = keras.Input(shape=(64,64,64,1), name='full_input')\n",
    "new_encoded_1 = apos1.layers[1](full_input)\n",
    "new_encoded_2 = apos1.layers[2](new_encoded_1)\n",
    "new_encoded_3 = apos1.layers[3](new_encoded_2)\n",
    "new_encoded_4 = apos1.layers[4](new_encoded_3)\n",
    "\n",
    "atomencoder = keras.Model(full_input, new_encoded_4)\n",
    "\n",
    "#Now Decode \n",
    "encoded_input = keras.Input(shape=(512,), name='encoded_input')\n",
    "new_decoded_0 = apos1.layers[5](encoded_input)\n",
    "new_decoded_1 = apos1.layers[6](new_decoded_0)\n",
    "new_decoded_2 = apos1.layers[7](new_decoded_1)\n",
    "new_decoded_3 = apos1.layers[8](new_decoded_2)\n",
    "new_decoded_4 = apos1.layers[9](new_decoded_3)\n",
    "new_decoded_5 = apos1.layers[10](new_decoded_4)\n",
    "new_decoded_6 = apos1.layers[11](new_decoded_5)\n",
    "#new_decoded_7 = apos.layers[12](new_decoded_6)\n",
    "\n",
    "atomdecoder = keras.Model(encoded_input, new_decoded_6)\n",
    "\n",
    "#apos = keras.models.load_model('Lattice_Encoder.keras', compile=False)\n",
    "full_input = keras.Input(shape=(32,32,32,1), name='full_input')\n",
    "new_encoded_1 = apos.layers[1](full_input)\n",
    "new_encoded_2 = apos.layers[2](new_encoded_1)\n",
    "new_encoded_3 = apos.layers[3](new_encoded_2)\n",
    "new_encoded_4 = apos.layers[4](new_encoded_3)\n",
    "\n",
    "latencoder = keras.Model(full_input, new_encoded_4)\n",
    "\n",
    "#Now Decode \n",
    "encoded_input = keras.Input(shape=(512,), name='encoded_input')\n",
    "new_decoded_0 = apos.layers[5](encoded_input)\n",
    "new_decoded_1 = apos.layers[6](new_decoded_0)\n",
    "new_decoded_2 = apos.layers[7](new_decoded_1)\n",
    "new_decoded_3 = apos.layers[8](new_decoded_2)\n",
    "new_decoded_4 = apos.layers[9](new_decoded_3)\n",
    "new_decoded_5 = apos.layers[10](new_decoded_4)\n",
    "\n",
    "latdecoder = keras.Model(encoded_input, new_decoded_5)\n",
    "\n",
    "\n",
    "def format_output(out):\n",
    "    n=np.shape(out)[0]\n",
    "    out=2*out-1\n",
    "    atoms=out[:,1:,:]\n",
    "    lats=out[:,:1,:]\n",
    "    return lats, atoms\n",
    "\n",
    "def elements(decoded,tol1):\n",
    "    atm_lst=[]\n",
    "    for j in range(1,79):\n",
    "\n",
    "        prob=(np.max(decoded[j,:,:,:,0]))\n",
    "        #print(prob)\n",
    "        if prob>tol1:\n",
    "            atm_lst.append(j)\n",
    "    return atm_lst\n",
    "#Make a list with atom locations\n",
    "def atmloc(decoded,atmnum,tol):\n",
    "    atomlocation=[]\n",
    "    jj=int(atmnum)\n",
    "    #for jj in [atmnum]:\n",
    "    testa=decoded[jj:jj+1,:,:,:,:]\n",
    "    #print(np.shape(test))\n",
    "    a=testa[0,:,:,:,0]\n",
    "    #a=img[2001]\n",
    "    sorted_index_array = np.argsort((a.flatten()))\n",
    "    sorted_array = (a.flatten())[sorted_index_array]\n",
    "    rslt = sorted_array[-10000 : ]\n",
    "    atom_coords=[]\n",
    "    found=False\n",
    "    j=1\n",
    "    while found==False:\n",
    "        if rslt[-j]>tol:\n",
    "            crd= np.where(a==rslt[-j])\n",
    "            ijk=[crd[0][0],crd[1][0],crd[2][0]]\n",
    "            if j==1:\n",
    "                atom_coords.append(ijk)\n",
    "            #if num_atoms==1:\n",
    "            #    found=True\n",
    "            else:\n",
    "                dist=[]\n",
    "                for point in atom_coords:\n",
    "                    \n",
    "                    dist.append(np.linalg.norm(np.array(ijk)-np.array(point))*(12/64))\n",
    "                    #Introduce Periodicity here\n",
    "                if np.min(dist)>2:\n",
    "                    atom_coords.append(ijk)\n",
    "            #if (np.shape(np.array(atom_coords))[0])==int(num_atoms):\n",
    "            #    found=True\n",
    "        j=j+1\n",
    "        if j==10000:\n",
    "            atomlocation.append(np.array(atom_coords)*(12/64))\n",
    "            found=True\n",
    "    return atomlocation\n",
    "\n",
    "def extract_lat(latdecoded):\n",
    "    a=(-np.log(latdecoded[0,-1,0,0,0])*2) #a\n",
    "    b=(-np.log(latdecoded[0,0,-1,0,0])*2) #b\n",
    "    c=(-np.log(latdecoded[0,0,0,-1,0])*2)+15 #c\n",
    "    g=((np.arccos((-np.log(latdecoded[0,-1,-1,0,0])*2-a-b)/(a*b)))*(180/np.pi))\n",
    "    return a, b, c, g\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import chgnet\n",
    "from chgnet.model import CHGNet\n",
    "from chgnet.model import StructOptimizer\n",
    "chgnet = CHGNet.from_file('01-08-2025/epoch19_e410_f282_sNA_mNA.pth.tar')\n",
    "relaxer = StructOptimizer()\n",
    "# Perturb the structure\n",
    "def PhononDisc(xf, i1, i2):\n",
    "    size=np.shape(xf)[0]\n",
    "    samples=xf\n",
    "    Res1=np.zeros(size)\n",
    "    Res2=np.zeros(size)\n",
    "    lats, atoms = format_output(samples)\n",
    "    tol=0.3\n",
    "    tol1=0.3\n",
    "    for n in range(size):\n",
    "        #try:\n",
    "        #print(n)\n",
    "        decodedatoms=atomdecoder.predict(atoms[n])\n",
    "        #print(np.max(decodedatoms))\n",
    "        atm_lst = elements(decodedatoms,tol1)\n",
    "        #print(atm_lst)\n",
    "        atm_names=[]\n",
    "        atm_crd=[]\n",
    "        if np.shape(atm_lst)[0]<20 and np.shape(atm_lst)[0]>1:\n",
    "            for k1 in atm_lst:\n",
    "                atom_location= atmloc(decodedatoms,k1,tol)\n",
    "                for k2 in range(np.shape(atom_location)[1]):\n",
    "                    atm_names.append(atom_dict[k1])\n",
    "                    atm_crd.append(atom_location[0][k2])\n",
    "            if np.shape(atm_names)[0]<20:\n",
    "                latdecoded=latdecoder.predict(lats[n])\n",
    "                a1, b1, c1, g=extract_lat(latdecoded)\n",
    "                #Now build structure in pymatgen\n",
    "                coords = atm_crd\n",
    "                names=atm_names\n",
    "                lattice = Lattice.from_parameters(a=a1, b=b1, c=c1, alpha=90,\n",
    "                                                  beta=90, gamma=g)\n",
    "                struct = Structure(lattice, names, coords,coords_are_cartesian=True)\n",
    "                dist=struct.distance_matrix\n",
    "                atom_cut=np.min(dist+10*np.diag(np.ones(np.shape(dist)[0])),axis=0)\n",
    "                check_d=np.shape(dist)[0]**2-np.count_nonzero(np.heaviside(np.abs(dist+2*np.diag(np.ones(np.shape(dist)[0])))-1,0))\n",
    "                if check_d==0 and np.max(atom_cut)<6:\n",
    "                    print(struct)\n",
    "                    struct.perturb(0.1)\n",
    "                    result = relaxer.relax(struct, verbose=False)\n",
    "                    struct=result[\"final_structure\"]\n",
    "                    phonon_flow = PhononMaker(min_length=15.0, store_force_constants=False).make(structure=struct)\n",
    "                    run_locally(phonon_flow, create_folders=True)\n",
    "                    store = SETTINGS.JOB_STORE\n",
    "                    store.connect()\n",
    "                    result = store.query_one(\n",
    "                        {\"name\": \"generate_frequencies_eigenvectors\"},\n",
    "                        properties=[\n",
    "                            \"output.phonon_dos\",\n",
    "                            \"output.phonon_bandstructure\",\n",
    "                        ],\n",
    "                        load=True,\n",
    "                        sort={\"completed_at\": -1} # to get the latest computation\n",
    "                    )\n",
    "                    ph_dos = PhononDos.from_dict(result['output']['phonon_dos']) # get pymatgen phonon dos object\n",
    "                    #stable=[False,True][int(np.heaviside(1-np.sum(np.abs(np.heaviside(-ph_dos.frequencies,0)*ph_dos.densities)),0))]\n",
    "                    stable=1-np.clip(np.sum(np.heaviside(-ph_dos.frequencies,0)*np.clip(ph_dos.densities,0,.05)),0,1)\n",
    "                    print('Phonon Negative Densities: '+str(stable))\n",
    "                    Res1[n]=stable\n",
    "                    os.mkdir('Temp')\n",
    "                    subprocess.call('scp atom_init.json Temp', shell=True)\n",
    "                    subprocess.call('scp id_prop.csv Temp', shell=True)\n",
    "                    struct.to(filename=\"./Temp/temp.cif\")\n",
    "                    subprocess.call('python predict.py model_best.pth.tar ../Temp', shell=True, cwd='./cgcnn-master')\n",
    "                    gap_res = genfromtxt('cgcnn-master/test_results.csv', delimiter=',')\n",
    "                    subprocess.call('rm -r Temp', shell=True)\n",
    "                    subprocess.call('rm -rf job_*', shell=True)\n",
    "                    print(\"Predicted band gap: \"+str(gap_res[2]))\n",
    "                    Res2[n]=np.clip((gap_res[2])+.5,0,1)\n",
    "                    if gap_res[2]>0 and stable>0:\n",
    "                        #Res[n]=1\n",
    "                        name=str(struct.formula).replace(\" \", \"\")\n",
    "                        #struct.to(filename=\"./Final_CIFS/Untrained/Good_Ins_\"+str(n)+\"_\"+str(i1)+\"_\"+str(i2)+\"_\"+str(name)+\".cif\")\n",
    "                        \n",
    "    return Res1, Res2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Cgx7W3sB17S4"
   },
   "outputs": [],
   "source": [
    "#Overfit on results of phonon density of states + CGCCNN gap prediction to use as discriminator \n",
    "def define_mlp_stab_discriminator():\n",
    "    input_img0 = keras.Input(shape=( 80, 512, 1))\n",
    "    x = layers.Conv2D(4, 2, strides = (2,4),  padding='same')(input_img0)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 2,strides = (2,2),  padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 3, strides = (2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    output= layers.Dense(1, activation='sigmoid')(x)\n",
    "    mlp_disc_stab = keras.Model(input_img0, output)\n",
    "    mlp_disc_stab.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #crstldisc.summary()\n",
    "    return mlp_disc_stab\n",
    "def define_mlp_gap_discriminator():\n",
    "    input_img1 = keras.Input(shape=( 80, 512, 1))\n",
    "    x = layers.Conv2D(4, 2, strides = (2,4),  padding='same')(input_img1)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 2,strides = (2,2),  padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 3, strides = (2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    output= layers.Dense(1, activation='sigmoid')(x)\n",
    "    mlp_disc_gap = keras.Model(input_img1, output)\n",
    "    mlp_disc_gap.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #crstldisc.summary()\n",
    "    return mlp_disc_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, n_samples, x_input):\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X[:,:,:,0], y\n",
    "def define_gan_mlp(g_model, mlp_stability_disc, mlp_gap_disc):\n",
    "    # make weights in the discriminator not trainable\n",
    "    mlp_stability_disc.trainable = False\n",
    "    mlp_gap_disc.trainable = False\n",
    "    # connect them\n",
    "    input_img = keras.Input(shape=( 128,))\n",
    "    # add generator\n",
    "    gen = g_model(input_img)\n",
    "    # add disc\n",
    "    stab = mlp_stability_disc(gen)\n",
    "    gap = mlp_gap_disc(gen)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "    model = keras.Model(inputs=input_img, outputs=[stab,gap])\n",
    "    model.compile(loss=['binary_crossentropy','binary_crossentropy'], optimizer=opt, metrics=['accuracy','accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "kuuPx1W-ekQ1"
   },
   "outputs": [],
   "source": [
    "def train(g_model):\n",
    "    half_batch = int(64)\n",
    "    g2_loss=[]\n",
    "    #g2_loss=np.load('MLP_loss.npy')\n",
    "    #g2_loss=g2_loss.tolist()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(150,200):\n",
    "        X_gan=generate_latent_points(128,half_batch)\n",
    "        ##########################################################\n",
    "        # generate 'fake' examples\n",
    "        g_model=keras.models.load_model('g_model_mlp.keras')\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, half_batch, X_gan)\n",
    "        ##########################################################\n",
    "        #Check for dynamic stability\n",
    "        res1, res2 =PhononDisc(X_fake, i, 0)\n",
    "        mlp_stability_disc=define_mlp_stab_discriminator()\n",
    "        mlp_stability_disc=keras.models.load_model('mlp_stab.keras')\n",
    "        mlp_gap_disc=define_mlp_gap_discriminator()\n",
    "        mlp_gap_disc=keras.models.load_model('mlp_gap.keras')\n",
    "        Y1=np.expand_dims(np.array(res1),1)\n",
    "        mlp_stability_disc.fit(X_fake,Y1,batch_size=64,epochs=300,verbose=0)\n",
    "        Y2=np.expand_dims(np.array(res2),1)\n",
    "        mlp_gap_disc.fit(X_fake,Y2,batch_size=64,epochs=300,verbose=0)\n",
    "        ##########################################################\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((half_batch, 1))\n",
    "        #Train w/ MLP\n",
    "        mlp_stability_disc.trainable = False\n",
    "        mlp_gap_disc.trainable = False\n",
    "        # connect them\n",
    "        input_img = keras.Input(shape=( 128,))\n",
    "        # add generator\n",
    "        gen = g_model(input_img)\n",
    "        # add disc\n",
    "        stab = mlp_stability_disc(gen)\n",
    "        gap = mlp_gap_disc(gen)\n",
    "        # compile model\n",
    "        opt = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "        gan_model2 = keras.Model(inputs=input_img, outputs=[stab,gap])\n",
    "        gan_model2.compile(loss=['binary_crossentropy','binary_crossentropy'], optimizer=opt, metrics=['accuracy','accuracy'])\n",
    "        #gan_model2 = define_gan2(g_model,mlp_stability_disc, mlp_gap_disc)\n",
    "        g_loss2= gan_model.train_on_batch(X_gan, [y_gan, y_gan])\n",
    "        print(\"GAN Loss: \"+str(g_loss2))\n",
    "        g2_loss.append(g_loss2)\n",
    "        np.save('MLP_loss.npy',np.array(g2_loss))\n",
    "        print('Epoch '+str(i))\n",
    "        g_model.save('g_model_mlp.keras')\n",
    "        #save models for this epoch \n",
    "        g_model.save('g_model_mlp_Epoch_'+str(i)+'.keras')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_model = define_generator()\n",
    "#g_model=keras.models.load_model('g_model_mlp_Epoch_50.keras')\n",
    "g_model.save('g_model_mlp.keras')\n",
    "mlp_stability_disc=define_mlp_stab_discriminator()\n",
    "mlp_stability_disc.save('mlp_stab.keras')\n",
    "mlp_gap_disc=define_mlp_gap_discriminator()\n",
    "mlp_gap_disc.save('mlp_gap.keras')\n",
    "train(g_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
