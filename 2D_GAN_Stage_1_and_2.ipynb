{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training phase 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpJhs8zTjtPC",
    "outputId": "3863286b-7628-4cf8-b122-b7ae4875bb4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Input\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import UpSampling2D\n",
    "import keras\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VS3DjudcwZ4R"
   },
   "outputs": [],
   "source": [
    "# creates: band-alignment.png\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pymatgen.core import Lattice, Structure, Molecule\n",
    "from pymatgen.analysis.structure_matcher import StructureMatcher\n",
    "#from ase.db import connect\n",
    "#import ase\n",
    "atom_dict1=['na','H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm','Md','No','Lr','Rf','Db','Sg','Bh','Hs','Mt','Ds','Rg','Cn','Nh','Fl','Mc','Lv','Ts','Og']\n",
    "#removing nobels \n",
    "atom_dict=['na','H','Li','Be','B','C','N','O','F','Na','Mg','Al','Si','P','S','Cl','K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru','Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb','Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn','Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm','Md','No','Lr','Rf','Db','Sg','Bh','Hs','Mt','Ds','Rg','Cn','Nh','Fl','Mc','Lv','Ts','Og']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw1RWSXJEvDC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5r22aYaQzdbi"
   },
   "outputs": [],
   "source": [
    "## Load Real crystal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le5VzkTkO0t6",
    "outputId": "79825d99-b99b-4e25-83f5-afc7ee050d1d"
   },
   "outputs": [],
   "source": [
    "x_real_train=np.load('2D_Real_Train.npy')\n",
    "#np.shape(x_real_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrVdZKAtsP_s"
   },
   "source": [
    "Load Topo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "K6h_uzCCsrJD"
   },
   "outputs": [],
   "source": [
    "x_train_topo=np.load('x_train_topo.npy')\n",
    "y_train_topo=np.load('y_train_topo.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AMBj2Yd8s8UF"
   },
   "outputs": [],
   "source": [
    "def generate_topo_samples(xdataset, ydataset, n_samples):\n",
    "    ix= randint(0, xdataset.shape[0], n_samples)\n",
    "    X=xdataset[ix]\n",
    "    y=ydataset[ix]\n",
    "    y = np.expand_dims(y,1)\n",
    "    #y = np.ones((n_samples,1))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hhHjKt9Ln2kZ"
   },
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vWrXBGT5tgoS"
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix= randint(0, dataset.shape[0], n_samples)\n",
    "    X=dataset[ix]\n",
    "    y = np.ones((n_samples,1))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OH3mwUyzYMle"
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(n_samples):\n",
    "    X=generate_latent_points(512*80,n_samples)\n",
    "    X=X.reshape((n_samples,160,256))/(np.max(np.abs(X)))\n",
    "    X=(X+1)/2\n",
    "    y = np.zeros((n_samples,1))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gHHfpck5YclQ"
   },
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    input_img = keras.Input(shape=( 80, 512, 1))\n",
    "    x = layers.Conv2D(4, 2, strides = (2,4),  padding='same')(input_img)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 2,strides = (2,2),  padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 3, strides = (2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    output= layers.Dense(1, activation='sigmoid')(x)\n",
    "    crstldisc = keras.Model(input_img, output)\n",
    "    crstldisc.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #crstldisc.summary()\n",
    "    return crstldisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "q5pkrQvm_gXc"
   },
   "outputs": [],
   "source": [
    "def define_topo_discriminator():\n",
    "    input_img = keras.Input(shape=( 80, 512, 1))\n",
    "    x = layers.Conv2D(4, 2, strides = (2,4),  padding='same')(input_img)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 2,strides = (2,2),  padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(32, 3, strides = (2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    output= layers.Dense(1, activation='sigmoid')(x)\n",
    "    crstldisc_topo = keras.Model(input_img, output)\n",
    "    crstldisc_topo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #crstldisc.summary()\n",
    "    return crstldisc_topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9ANzO_8ybTwi"
   },
   "outputs": [],
   "source": [
    "def define_generator():\n",
    "    #Generator\n",
    "    visible = keras.layers.Input(shape=(128,))\n",
    "    n_nodes = 128 * 16 * 10\n",
    "    dense = keras.layers.Dense(n_nodes)(visible)\n",
    "    reshape1 = keras.layers.Reshape((10, 16, 128))(dense)\n",
    "    upsample1 = keras.layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(reshape1)\n",
    "    LeakyRelu1 = keras.layers.LeakyReLU(alpha=0.2)(upsample1)\n",
    "    upsample2 = keras.layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(LeakyRelu1)\n",
    "    LeakyRelu2 = keras.layers.LeakyReLU(alpha=0.2)(upsample2)\n",
    "    upsample3 = keras.layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(LeakyRelu2)\n",
    "    LeakyRelu3 = keras.layers.LeakyReLU(alpha=0.2)(upsample3)\n",
    "    upsample4 = keras.layers.Conv2DTranspose(64, (2,2), strides=(1,4), padding='same')(LeakyRelu3)\n",
    "    LeakyRelu4 = keras.layers.LeakyReLU(alpha=0.2)(upsample4)\n",
    "    conv_layer1 = Conv2D(1, (2,2), activation='sigmoid', padding='same')(LeakyRelu4)\n",
    "    #output = crstldisc(conv_layer1)\n",
    "    generator = keras.Model(visible, conv_layer1)\n",
    "    generator.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    #generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AZ-kcVZ6ahcC"
   },
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(128,n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X[:,:,:,0], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aGbyMtwyeI3N"
   },
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    input_img = keras.Input(shape=( 128,))\n",
    "    # add generator\n",
    "    gen = g_model(input_img)\n",
    "    # add the discriminator\n",
    "    disc = d_model(gen)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0005, beta_1=0.5)\n",
    "    model = keras.Model(inputs=input_img, outputs=[disc])\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=opt)\n",
    "    return model\n",
    "def define_gan_topo(g_model, dtopo_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    dtopo_model.trainable = False\n",
    "    # connect them\n",
    "    input_img = keras.Input(shape=( 128,))\n",
    "    # add generator\n",
    "    gen = g_model(input_img)\n",
    "    # add the discriminator\n",
    "    topo = dtopo_model(gen)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model = keras.Model(inputs=input_img, outputs=[topo])\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvdpZP4sSc0K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cgx7W3sB17S4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, n_samples, x_input):\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X[:,:,:,0], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kuuPx1W-ekQ1"
   },
   "outputs": [],
   "source": [
    "def train(g_model, d_model, dtopo_model, dataset, xtopodataset, ytopodataset, n_epochs=200, n_batch=256):\n",
    "    half_batch = int(n_batch / 2)\n",
    "    g1_loss=[]\n",
    "    g2_loss=[]\n",
    "    # manually enumerate epochs\n",
    "    for i in range(10):\n",
    "    # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            X_gan=generate_latent_points(128,half_batch)\n",
    "            ##########################################################\n",
    "            # get randomly selected 'real' samples\n",
    "            if i<99:\n",
    "                X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "                # generate 'fake' examples\n",
    "                #g_model=keras.models.load_model('g_model4.keras')\n",
    "                X_fake, y_fake = generate_fake_samples(g_model, half_batch, X_gan)\n",
    "                # create training set for the discriminator\n",
    "                X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "                # update discriminator model weights\n",
    "                if i>0 or j>0:\n",
    "                    d_model=keras.models.load_model('d_model4.keras')\n",
    "                d_loss, d_acc = d_model.train_on_batch(X, y)\n",
    "                d_model.save('d_model.keras')\n",
    "            ##########################################################\n",
    "            # gen real topo samples\n",
    "            if i>99:\n",
    "                X_real, y_real = generate_topo_samples(xtopodataset, ytopodataset, half_batch)\n",
    "                # create training set for the discriminator\n",
    "                X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "                #update discriminator model weights\n",
    "                if i>0 or j>0:\n",
    "                    dtopo_model=keras.models.load_model('dtopo_model.keras')\n",
    "                d_losstopo, d_acctopo = dtopo_model.train_on_batch(X, y)\n",
    "                dtopo_model.save('dtopo_model4keras')\n",
    "            ##########################################################\n",
    "            # create inverted labels for the fake samples\n",
    "            if j>0:\n",
    "                g_model=keras.models.load_model('g_model.keras')\n",
    "            #Train Generator, phase I\n",
    "            if i<100:\n",
    "                gan_model = define_gan(g_model, d_model)\n",
    "            if i>=100:\n",
    "                gan_model = define_gan_topo(g_model, dtopo_model)\n",
    "            #Train Topology\n",
    "            y_gan = np.ones((half_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss1 = gan_model1.train_on_batch(X_gan, [y_gan])\n",
    "            print(\"GAN_1 Loss: \"+str(g_loss1))\n",
    "            g1_loss.append(g_loss1)\n",
    "            np.save('g1_loss.npy',np.array(g1_loss))\n",
    "            g_model.save('g_model.keras')\n",
    "        #Archive models evry 10 epochs\n",
    "        if i%10==0\n",
    "            g_model.save('g_model_Epoch_'+str(i)+'.keras')\n",
    "       \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rat changed to .0001\n",
    "g_model = define_generator()\n",
    "#g_model=keras.models.load_model('g_model_Epoch_9.keras')\n",
    "g_model.save('g_model.keras')\n",
    "d_model = define_discriminator()\n",
    "#d_model=keras.models.load_model('d_model_Epoch_9.keras')\n",
    "dtopo_model = define_topo_discriminator()\n",
    "#dtopo_model=keras.models.load_model('dtopo_model_Epoch_109.keras')\n",
    "#dtopo_model.save('dtopo_model.keras')\n",
    "traint(g_model, d_model, dtopo_model, x_real_train, x_train_topo, y_train_topo, n_epochs=200, n_batch=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
